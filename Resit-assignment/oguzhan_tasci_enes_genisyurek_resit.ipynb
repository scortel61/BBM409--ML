{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468923d9",
   "metadata": {},
   "source": [
    "# Resit Assignment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6fcc1c7",
   "metadata": {},
   "source": [
    "Group Member: Oğuzhan Taşçı              Student No: 2200356842\n",
    "Group Member: İbrahim Enes Genişyürek    Student No: 21892757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66313e",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2214e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacc087",
   "metadata": {},
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057f8357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Laura</td>\n",
       "      <td>crime</td>\n",
       "      <td>Like Wilkie Collins' groundbreaking detective...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Split Image</td>\n",
       "      <td>crime</td>\n",
       "      <td>The novel begins with Chief Stone investigati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You Are Not Alone</td>\n",
       "      <td>thriller</td>\n",
       "      <td>Shay Miller wants to find love, but it eludes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Artemis Fowl: The Arctic Incident</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Artemis Fowl II is the thirteen-year-old son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Under a Monsoon Cloud</td>\n",
       "      <td>crime</td>\n",
       "      <td>Inspector Ghote is temporarily assigned to a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title     genre  \\\n",
       "0                              Laura     crime   \n",
       "1                        Split Image     crime   \n",
       "2                  You Are Not Alone  thriller   \n",
       "3  Artemis Fowl: The Arctic Incident   fantasy   \n",
       "4              Under a Monsoon Cloud     crime   \n",
       "\n",
       "                                             summary  \n",
       "0   Like Wilkie Collins' groundbreaking detective...  \n",
       "1   The novel begins with Chief Stone investigati...  \n",
       "2  Shay Miller wants to find love, but it eludes ...  \n",
       "3   Artemis Fowl II is the thirteen-year-old son ...  \n",
       "4   Inspector Ghote is temporarily assigned to a ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"book_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab772d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crime', 'thriller', 'fantasy', 'horror', 'history', 'science'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7530b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "crime       500\n",
       "fantasy     500\n",
       "history     500\n",
       "horror      500\n",
       "science     500\n",
       "thriller    500\n",
       "Name: (title, count), dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('genre').describe().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81e238",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cacc42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a7b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1091bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessor(text):\n",
    "    # Keep only English letters and spaces\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed14f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train_data' and 'test_data' are DataFrames with columns 'Genre' and 'Summary'\n",
    "train_summary = train_data['summary'].values\n",
    "test_summary = test_data['summary'].values\n",
    "train_genre = train_data['genre'].values\n",
    "test_genre = test_data['genre'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f1ba9",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5860d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a CountVectorizer with a custom preprocessor for unigram features\n",
    "vectorizer_unigram = CountVectorizer(preprocessor=custom_preprocessor)\n",
    "X_train_unigram = vectorizer_unigram.fit_transform(train_summary)\n",
    "X_test_unigram = vectorizer_unigram.transform(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d61acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a CountVectorizer with a custom preprocessor for bigram features\n",
    "vectorizer_bigram = CountVectorizer(preprocessor=custom_preprocessor, analyzer='word', ngram_range=(2, 2))\n",
    "X_train_bigram = vectorizer_bigram.fit_transform(train_summary)\n",
    "X_test_bigram = vectorizer_bigram.transform(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2dc4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a CountVectorizer with a custom preprocessor for TD-IDF features\n",
    "tfidf_transformer = TfidfVectorizer(preprocessor=custom_preprocessor)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(train_summary)\n",
    "X_test_tfidf = tfidf_transformer.transform(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "835915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a CountVectorizer with a custom preprocessor for Unigram with Removed Stopwords features\n",
    "vectorizer_unigram_stop = CountVectorizer(preprocessor=custom_preprocessor, stop_words=stop_words_list)\n",
    "X_train_unigram_stop = vectorizer_unigram_stop.fit_transform(train_summary)\n",
    "X_test_unigram_stop = vectorizer_unigram_stop.transform(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45770e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a CountVectorizer with a custom preprocessor for Bigram With Removed Stopwords features\n",
    "vectorizer_bigram_stop = CountVectorizer(preprocessor=custom_preprocessor, analyzer='word', ngram_range=(2, 2),stop_words=stop_words_list)\n",
    "X_train_bigram_stop = vectorizer_bigram_stop.fit_transform(train_summary)\n",
    "X_test_bigram_stop = vectorizer_bigram_stop.transform(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72889cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a CountVectorizer with a custom preprocessor for TF-IDF With Removed Stopwords features\n",
    "tfidf_transformer_stop = TfidfVectorizer(preprocessor=custom_preprocessor, stop_words=stop_words_list)\n",
    "X_train_tfidf_stop = tfidf_transformer_stop.fit_transform(train_summary)\n",
    "X_test_tfidf_stop = tfidf_transformer_stop.transform(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b0e6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_train(X_train, y_train):\n",
    "    num_samples, num_features = X_train.shape\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    word_counts = np.zeros((num_classes, num_features))\n",
    "    \n",
    "    # Calculate class and word counts\n",
    "    for i in range(num_samples):\n",
    "        class_counts[y_train[i]] += 1\n",
    "        word_counts[y_train[i]] += X_train[i]\n",
    "    \n",
    "    # Laplace smoothing\n",
    "    alpha = 1\n",
    "    class_priors = (class_counts + alpha) / (num_samples + num_classes * alpha)\n",
    "    word_probs = (word_counts + alpha) / (np.sum(word_counts, axis=1, keepdims=True) + num_features * alpha)\n",
    "    \n",
    "    # Take logarithm\n",
    "    log_class_priors = np.log(class_priors)\n",
    "    log_word_probs = np.log(word_probs)\n",
    "    \n",
    "    return log_class_priors, log_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9722424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(X_test, log_class_priors, log_word_probs):\n",
    "    num_samples, num_classes = X_test.shape[0], log_class_priors.shape[0]\n",
    "    log_probs = np.zeros((num_samples, num_classes))\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    for i in range(num_samples):\n",
    "        log_probs[i] = log_class_priors + np.sum(X_test[i] * log_word_probs, axis=1)\n",
    "    \n",
    "    # Predict class with maximum log probability\n",
    "    predictions = np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd73a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert genre labels to integers\n",
    "genre_mapping = {'crime': 0, 'thriller': 1, 'fantasy': 2, 'horror': 3, 'history': 4, 'science': 5}\n",
    "train_genre_int = np.array([genre_mapping[genre] for genre in train_genre])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c604a7",
   "metadata": {},
   "source": [
    "### Functions that compute Accuracy, Precision and Recall For Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d858e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Accuracy function\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_samples = len(y_true)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7ca3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Precision function\n",
    "def calculate_precision(y_true, y_pred, positive_class):\n",
    "    true_positives = np.sum((y_true == positive_class) & (y_pred == positive_class))\n",
    "    predicted_positives = np.sum(y_pred == positive_class)\n",
    "    \n",
    "    if predicted_positives == 0:\n",
    "        return 0  # To avoid division by zero\n",
    "    \n",
    "    precision = true_positives / predicted_positives\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b725a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Recall function\n",
    "def calculate_recall(y_true, y_pred, positive_class):\n",
    "    true_positives = np.sum((y_true == positive_class) & (y_pred == positive_class))\n",
    "    actual_positives = np.sum(y_true == positive_class)\n",
    "    \n",
    "    if actual_positives == 0:\n",
    "        return 0  # To avoid division by zero\n",
    "    \n",
    "    recall = true_positives / actual_positives\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b4df9",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9da7fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using unigram features\n",
    "class_priors_unigram, word_probs_unigram = naive_bayes_train(X_train_unigram.toarray(), train_genre_int)\n",
    "predictions_unigram = naive_bayes_predict(X_test_unigram.toarray(), class_priors_unigram, word_probs_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f35a9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_int = np.array([genre_mapping[genre] for genre in test_genre])\n",
    "predicted_labels_int = predictions_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d7c949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, and recall for unigram\n",
    "accuracy_unigram = calculate_accuracy(ground_truth_int, predicted_labels_int)\n",
    "precision_unigram = calculate_precision(ground_truth_int, predicted_labels_int, positive_class=1)\n",
    "recall_unigram = calculate_recall(ground_truth_int, predicted_labels_int, positive_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc4dcec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.64\n",
      "Recall: 0.48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_unigram:.2f}\")\n",
    "print(f\"Precision: {precision_unigram:.2f}\")\n",
    "print(f\"Recall: {recall_unigram:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b734f",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3960ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using bigram features\n",
    "class_priors_bigram, word_probs_bigram = naive_bayes_train(X_train_bigram.toarray(), train_genre_int)\n",
    "predictions_bigram = naive_bayes_predict(X_test_bigram.toarray(), class_priors_bigram, word_probs_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd9f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_int = predictions_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1226d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, and recall for bigram\n",
    "accuracy_bigram = calculate_accuracy(ground_truth_int, predicted_labels_int)\n",
    "precision_bigram = calculate_precision(ground_truth_int, predicted_labels_int, positive_class=1)\n",
    "recall_bigram = calculate_recall(ground_truth_int, predicted_labels_int, positive_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a27d0fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63\n",
      "Precision: 0.75\n",
      "Recall: 0.20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_bigram:.2f}\")\n",
    "print(f\"Precision: {precision_bigram:.2f}\")\n",
    "print(f\"Recall: {recall_bigram:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab3536",
   "metadata": {},
   "source": [
    "### TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "822b4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using TD-IDF features\n",
    "class_priors_tfidf, word_probs_tfidf = naive_bayes_train(X_train_tfidf.toarray(), train_genre_int)\n",
    "predictions_tfidf = naive_bayes_predict(X_test_tfidf.toarray(), class_priors_tfidf, word_probs_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3942e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_int = predictions_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c6e0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, and recall for TD-IDF\n",
    "accuracy_tfidf = calculate_accuracy(ground_truth_int, predicted_labels_int)\n",
    "precision_tfidf = calculate_precision(ground_truth_int, predicted_labels_int, positive_class=1)\n",
    "recall_tfidf = calculate_recall(ground_truth_int, predicted_labels_int, positive_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c728772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.55\n",
      "Recall: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_tfidf:.2f}\")\n",
    "print(f\"Precision: {precision_tfidf:.2f}\")\n",
    "print(f\"Recall: {recall_tfidf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ac98e",
   "metadata": {},
   "source": [
    "### Unigram With Stopwords Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61209788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using Unigram With Stopwords Removed features\n",
    "class_priors_unigram_stop, word_probs_unigram_stop = naive_bayes_train(X_train_unigram_stop.toarray(), train_genre_int)\n",
    "predictions_unigram_stop = naive_bayes_predict(X_test_unigram_stop.toarray(), class_priors_unigram_stop, word_probs_unigram_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b67317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_int = predictions_unigram_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a20f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, and recall for Unigram With Stopwords Removed\n",
    "accuracy_unigram_stop = calculate_accuracy(ground_truth_int, predicted_labels_int)\n",
    "precision_unigram_stop = calculate_precision(ground_truth_int, predicted_labels_int, positive_class=1)\n",
    "recall_unigram_stop = calculate_recall(ground_truth_int, predicted_labels_int, positive_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd68dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.65\n",
      "Recall: 0.54\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_unigram_stop:.2f}\")\n",
    "print(f\"Precision: {precision_unigram_stop:.2f}\")\n",
    "print(f\"Recall: {recall_unigram_stop:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09c1be",
   "metadata": {},
   "source": [
    "### Biagram With Stopwords Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1512f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using Bigram With Stopwords Removed features\n",
    "class_priors_bigram_stop, word_probs_bigram_stop = naive_bayes_train(X_train_bigram_stop.toarray(), train_genre_int)\n",
    "predictions_bigram_stop = naive_bayes_predict(X_test_bigram_stop.toarray(), class_priors_bigram_stop, word_probs_bigram_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05fcc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_int = predictions_bigram_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd26d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, and recall for Biagram With Stopwords Removed\n",
    "accuracy_bigram_stop = calculate_accuracy(ground_truth_int, predicted_labels_int)\n",
    "precision_bigram_stop = calculate_precision(ground_truth_int, predicted_labels_int, positive_class=1)\n",
    "recall_bigram_stop = calculate_recall(ground_truth_int, predicted_labels_int, positive_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43788c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "Precision: 0.48\n",
      "Recall: 0.47\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_bigram_stop:.2f}\")\n",
    "print(f\"Precision: {precision_bigram_stop:.2f}\")\n",
    "print(f\"Recall: {recall_bigram_stop:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1b4b5",
   "metadata": {},
   "source": [
    "### TD-IDF With Stopwords Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "575f2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using TD-IDF With Stopwords Removed features\n",
    "class_priors_tfidf_stop, word_probs_tfidf_stop = naive_bayes_train(X_train_tfidf_stop.toarray(), train_genre_int)\n",
    "predictions_tfidf_stop = naive_bayes_predict(X_test_tfidf_stop.toarray(), class_priors_tfidf_stop, word_probs_tfidf_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba59fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_int = predictions_tfidf_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a6d69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, and recall for TD-IDF With Stopwords Removed\n",
    "accuracy_tfidf_stop = calculate_accuracy(ground_truth_int, predicted_labels_int)\n",
    "precision_tfidf_stop = calculate_precision(ground_truth_int, predicted_labels_int, positive_class=1)\n",
    "recall_tfidf_stop = calculate_recall(ground_truth_int, predicted_labels_int, positive_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f54e4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Precision: 0.56\n",
      "Recall: 0.70\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_tfidf_stop:.2f}\")\n",
    "print(f\"Precision: {precision_tfidf_stop:.2f}\")\n",
    "print(f\"Recall: {recall_tfidf_stop:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1b4a5",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "677ddebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data['summary'].values\n",
    "genre = data['genre'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46573318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram\n",
    "X_unigram = vectorizer_unigram.fit_transform(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbc14038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biagram\n",
    "X_bigram = vectorizer_bigram.fit_transform(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df63976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "X_tfidf = tfidf_transformer.fit_transform(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d52101f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram with Removed Stopwords\n",
    "X_unigram_stop = vectorizer_unigram_stop.fit_transform(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "403dd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram With Removed Stopwords\n",
    "X_bigram_stop = vectorizer_bigram_stop.fit_transform(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e28bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF With Removed Stopwords\n",
    "X_tfidf_stop = tfidf_transformer_stop.fit_transform(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7855132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize centroids for k-means clustering\n",
    "def initialize_centroids(k, n_features):\n",
    "    return np.random.rand(k, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2bbcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Assign data points to the nearest cluster based on Euclidean distance.\n",
    "def assign_to_clusters(X, centroids):\n",
    "    distances = np.linalg.norm(X - centroids[:, np.newaxis], axis=2)\n",
    "    return np.argmin(distances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34bad538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update centroids based on the mean of data points in each cluster\n",
    "def update_centroids(X, clusters, k):\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        cluster_points = X[clusters == i]\n",
    "        if len(cluster_points) > 0:\n",
    "            centroids[i] = np.mean(cluster_points, axis=0)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19d899a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering on the input data.\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = initialize_centroids(k, n_features)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        clusters = assign_to_clusters(X, centroids)\n",
    "        new_centroids = update_centroids(X, clusters, k)\n",
    "\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6b4cf",
   "metadata": {},
   "source": [
    "### Functions that compute Accuracy, Precision and Recall For K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec71c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Accuracy function\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    correct_predictions = np.sum(true_labels == predicted_labels)\n",
    "    total_samples = len(true_labels)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "986f256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Precision function\n",
    "def calculate_precision(true_labels, predicted_labels, positive_class):\n",
    "    true_positive = np.sum((true_labels == positive_class) & (predicted_labels == positive_class))\n",
    "    false_positive = np.sum((true_labels != positive_class) & (predicted_labels == positive_class))\n",
    "    \n",
    "    if true_positive + false_positive == 0:\n",
    "        precision = 0  # Avoid division by zero\n",
    "    else:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14100271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Recall function\n",
    "def calculate_recall(true_labels, predicted_labels, positive_class):\n",
    "    true_positive = np.sum((true_labels == positive_class) & (predicted_labels == positive_class))\n",
    "    false_negative = np.sum((true_labels == positive_class) & (predicted_labels != positive_class))\n",
    "    \n",
    "    if true_positive + false_negative == 0:\n",
    "        recall = 0  # Avoid division by zero\n",
    "    else:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64ada3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert genre labels to integers\n",
    "genre_mapping = {'crime': 0, 'thriller': 1, 'fantasy': 2, 'horror': 3, 'history': 4, 'science': 5}\n",
    "ground_truth_int = np.array([genre_mapping[genre] for genre in genre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17b30c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to cluster into 6 genres\n",
    "k = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56a6e1",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aee864c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on Unigram\n",
    "clusters_unigram, centroids_unigram = kmeans(X_unigram.toarray(), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c45c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Unigram K-Means model\n",
    "accuracy_unigram_K = calculate_accuracy(ground_truth_int, clusters_unigram)\n",
    "precision_unigram_K = calculate_precision(ground_truth_int, clusters_unigram, positive_class=0)\n",
    "recall_unigram_K = calculate_recall(ground_truth_int, clusters_unigram, positive_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea394aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigram BoW): 0.169\n",
      "Precision (Unigram BoW): 0.16052631578947368\n",
      "Recall (Unigram BoW): 0.122\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (Unigram BoW):\", accuracy_unigram_K)\n",
    "print(\"Precision (Unigram BoW):\", precision_unigram_K)\n",
    "print(\"Recall (Unigram BoW):\", recall_unigram_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596c6be",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "676eb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on TF_IDF\n",
    "clusters_tfidf, centroids_tfidf = kmeans(X_tfidf.toarray(), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef74b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TF_IDF K-Means model\n",
    "accuracy_tfidf_K = calculate_accuracy(ground_truth_int, clusters_tfidf)\n",
    "precision_tfidf_K = calculate_precision(ground_truth_int, clusters_tfidf, positive_class=0)\n",
    "recall_tfidf_K = calculate_recall(ground_truth_int, clusters_tfidf, positive_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31b829fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigram BoW): 0.182\n",
      "Precision (Unigram BoW): 0.13204853675945752\n",
      "Recall (Unigram BoW): 0.37\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (Unigram BoW):\", accuracy_tfidf_K)\n",
    "print(\"Precision (Unigram BoW):\", precision_tfidf_K)\n",
    "print(\"Recall (Unigram BoW):\", recall_tfidf_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993a69e",
   "metadata": {},
   "source": [
    "### Unigram With Stopwords Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8e74aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram With Stopwords Removed\n",
    "clusters_unigram_stop, centroids_unigram_stop = kmeans(X_unigram_stop.toarray(), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8484f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Unigram With Stopwords Removed\n",
    "accuracy_unigram_stop_K = calculate_accuracy(ground_truth_int, clusters_unigram_stop)\n",
    "precision_unigram_stop_K = calculate_precision(ground_truth_int, clusters_unigram_stop, positive_class=0)\n",
    "recall_unigram_stop_K = calculate_recall(ground_truth_int, clusters_unigram_stop, positive_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd8a4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigram BoW): 0.148\n",
      "Precision (Unigram BoW): 0.15789473684210525\n",
      "Recall (Unigram BoW): 0.006\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (Unigram BoW):\", accuracy_unigram_stop_K)\n",
    "print(\"Precision (Unigram BoW):\", precision_unigram_stop_K)\n",
    "print(\"Recall (Unigram BoW):\", recall_unigram_stop_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac3aa8",
   "metadata": {},
   "source": [
    "### TF-IDF With Removed Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40dd6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on TF-IDF With Removed Stopwords\n",
    "clusters_tfidf_stop, centroids__tfidf_stop = kmeans(X_tfidf_stop.toarray(), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32062eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TF-IDF With Removed Stopwords\n",
    "accuracy_tfidf_stop_K = calculate_accuracy(ground_truth_int, clusters_tfidf_stop)\n",
    "precision_tfidf_stop_K = calculate_precision(ground_truth_int, clusters_tfidf_stop, positive_class=0)\n",
    "recall_tfidf_stop_K = calculate_recall(ground_truth_int, clusters_tfidf_stop, positive_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d725ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigram BoW): 0.172\n",
      "Precision (Unigram BoW): 0.20300751879699247\n",
      "Recall (Unigram BoW): 0.054\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (Unigram BoW):\", accuracy_tfidf_stop_K)\n",
    "print(\"Precision (Unigram BoW):\", precision_tfidf_stop_K)\n",
    "print(\"Recall (Unigram BoW):\", recall_tfidf_stop_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad59ca5",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12fc5759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature       | Stopwords | Algorithm | Accuracy | Precision | Recall\n",
      "---------------------------------------------------------------------------\n",
      "BoW (Unigram) |           |    NB     |   0.71   |   0.64    |  0.48\n",
      "---------------------------------------------------------------------------\n",
      "BoW (Unigram) | Removed   |    NB     |   0.63   |   0.75    |  0.20\n",
      "---------------------------------------------------------------------------\n",
      "BoW (Bigram)  |           |    NB     |   0.72   |   0.55    |  0.67\n",
      "---------------------------------------------------------------------------\n",
      "BoW (Bigram)  | Removed   |    NB     |   0.72   |   0.65    |  0.54\n",
      "---------------------------------------------------------------------------\n",
      "    TF-IDF    |           |    NB     |   0.61   |   0.48    |  0.47\n",
      "---------------------------------------------------------------------------\n",
      "    TF-IDF    | Removed   |    NB     |   0.73   |   0.56    |  0.70\n",
      "---------------------------------------------------------------------------\n",
      "BoW (Unigram) |           |  K-Means  |   0.17   |   0.16    |  0.12\n",
      "---------------------------------------------------------------------------\n",
      "BoW (Unigram) | Removed   |  K-Means  |   0.15   |   0.16    |  0.01\n",
      "---------------------------------------------------------------------------\n",
      "    TF-IDF    |           |  K-Means  |   0.18   |   0.13    |  0.37\n",
      "---------------------------------------------------------------------------\n",
      "    TF-IDF    | Removed   |  K-Means  |   0.17   |   0.20    |  0.05\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display results in a table format\n",
    "print(\"Feature       | Stopwords | Algorithm | Accuracy | Precision | Recall\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"BoW (Unigram) |           |    NB     |   {accuracy_unigram:.2f}   |   {precision_unigram:.2f}    |  {recall_unigram:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"BoW (Unigram) | Removed   |    NB     |   {accuracy_bigram:.2f}   |   {precision_bigram:.2f}    |  {recall_bigram:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"BoW (Bigram)  |           |    NB     |   {accuracy_tfidf:.2f}   |   {precision_tfidf:.2f}    |  {recall_tfidf:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"BoW (Bigram)  | Removed   |    NB     |   {accuracy_unigram_stop:.2f}   |   {precision_unigram_stop:.2f}    |  {recall_unigram_stop:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"    TF-IDF    |           |    NB     |   {accuracy_bigram_stop:.2f}   |   {precision_bigram_stop:.2f}    |  {recall_bigram_stop:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"    TF-IDF    | Removed   |    NB     |   {accuracy_tfidf_stop:.2f}   |   {precision_tfidf_stop:.2f}    |  {recall_tfidf_stop:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"BoW (Unigram) |           |  K-Means  |   {accuracy_unigram_K:.2f}   |   {precision_unigram_K:.2f}    |  {recall_unigram_K:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"BoW (Unigram) | Removed   |  K-Means  |   {accuracy_unigram_stop_K:.2f}   |   {precision_unigram_stop_K:.2f}    |  {recall_unigram_stop_K:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"    TF-IDF    |           |  K-Means  |   {accuracy_tfidf_K:.2f}   |   {precision_tfidf_K:.2f}    |  {recall_tfidf_K:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"    TF-IDF    | Removed   |  K-Means  |   {accuracy_tfidf_stop_K:.2f}   |   {precision_tfidf_stop_K:.2f}    |  {recall_tfidf_stop_K:.2f}\")\n",
    "print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5ddfd",
   "metadata": {},
   "source": [
    "### Naive Bayes (NB):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff638c39",
   "metadata": {},
   "source": [
    "Generally, removing stopwords tends to decrease accuracy but might increase precision.\n",
    "- Bigram BoW performs better than Unigram BoW in terms of accuracy and recall.\n",
    "- TF-IDF, in this case, doesn't show superior performance compared to BoW.\n",
    "- For NB, removing stopwords generally had a negative impact on accuracy. It led to a notable decrease in accuracy for BoW (Unigram) and a slight decrease for BoW (Bigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea9b3a",
   "metadata": {},
   "source": [
    "### K-Means:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab12536",
   "metadata": {},
   "source": [
    "- K-Means seems to perform poorly compared to Naive Bayes, especially with low accuracy and recall.\n",
    "- The removal of stopwords has a significant impact on K-Means performance, leading to lower accuracy.\n",
    "- Removing stopwords significantly decreased accuracy for K-Means, especially for BoW (Unigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18acaa9",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969f2a1",
   "metadata": {},
   "source": [
    "- Naive Bayes (NB) appears more suitable for this dataset, with better overall performance across different feature extraction methods.\n",
    "- Bigram BoW with Naive Bayes might be a good choice considering its higher accuracy and recall.\n",
    "- TF-IDF with Naive Bayes also shows promising results, especially with the stopwords included.\n",
    "- Removing stopwords does not consistently improve classification performance in this specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35048411",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54ea61",
   "metadata": {},
   "source": [
    "https://web.cs.hacettepe.edu.tr/~abc/teaching/bbm406/index.php\n",
    "\n",
    "https://stackoverflow.com\n",
    "\n",
    "https://theflyingmantis.medium.com/text-classification-in-nlp-naive-bayes-a606bf419f8c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
